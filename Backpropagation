import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load Iris dataset
data = load_iris()
X, y = data.data, data.target
y_one_hot = np.eye(3)[y]  # One-hot encoding for target labels

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)

# Hyperparameters
learning_rate, epochs, batch_size = 0.01, 7000, 16
input_size, hidden_size, output_size = 4, 6, 3
beta1, beta2, epsilon = 0.9, 0.999, 1e-8

# Xavier Initialization
np.random.seed(42)
W1, W2 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size), np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)

# Adam optimizer variables
m_w1, v_w1, m_w2, v_w2 = np.zeros_like(W1), np.zeros_like(W1), np.zeros_like(W2), np.zeros_like(W2)
t = 0

# Activation Functions
def leaky_relu(x, alpha=0.01): return np.where(x > 0, x, alpha * x)
def softmax(x): return np.exp(x - np.max(x, axis=1, keepdims=True)) / np.sum(np.exp(x - np.max(x, axis=1, keepdims=True)), axis=1, keepdims=True)
def cross_entropy_loss(y_pred, y_true): return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))
def accuracy(y_pred, y_true): return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))

# Training loop
losses, accuracies = [], []

for epoch in range(epochs):
    # Shuffle data
    indices = np.random.permutation(X_train.shape[0])
    X_train, y_train = X_train[indices], y_train[indices]
    
    total_loss = 0
    for i in range(0, X_train.shape[0], batch_size):
        # Get batch
        X_batch, y_batch = X_train[i:i+batch_size], y_train[i:i+batch_size]
        
        # Forward propagation
        A1 = leaky_relu(np.dot(X_batch, W1))
        A2 = softmax(np.dot(A1, W2))

        # Loss calculation
        loss = cross_entropy_loss(A2, y_batch)
        total_loss += loss
        
        # Backpropagation
        E2 = A2 - y_batch
        E1 = np.dot(E2, W2.T) * (A1 > 0)
        dW2, dW1 = np.dot(A1.T, E2) / batch_size, np.dot(X_batch.T, E1) / batch_size

        # Adam optimizer update
        t += 1
        m_w1, v_w1 = beta1 * m_w1 + (1 - beta1) * dW1, beta2 * v_w1 + (1 - beta2) * (dW1 ** 2)
        m_w2, v_w2 = beta1 * m_w2 + (1 - beta1) * dW2, beta2 * v_w2 + (1 - beta2) * (dW2 ** 2)

        # Bias correction
        m_w1_hat, v_w1_hat = m_w1 / (1 - beta1 ** t), v_w1 / (1 - beta2 ** t)
        m_w2_hat, v_w2_hat = m_w2 / (1 - beta1 ** t), v_w2 / (1 - beta2 ** t)

        # Weight update
        W1 -= learning_rate * m_w1_hat / (np.sqrt(v_w1_hat) + epsilon)
        W2 -= learning_rate * m_w2_hat / (np.sqrt(v_w2_hat) + epsilon)
    
    # Accuracy on test set
    A1_test = leaky_relu(np.dot(X_test, W1))
    A2_test = softmax(np.dot(A1_test, W2))
    acc = accuracy(A2_test, y_test)
    
    losses.append(total_loss / (X_train.shape[0] / batch_size))
    accuracies.append(acc)

    if epoch % 500 == 0:
        print(f"Epoch {epoch}: Loss={losses[-1]:.4f}, Accuracy={acc:.4f}")

# Plot results
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(losses, label="Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Over Time")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(accuracies, label="Accuracy", color="green")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Accuracy Over Time")
plt.legend()

plt.show()

# Final Test Accuracy
print(f"Final Test Accuracy: {accuracies[-1]:.4f}")
